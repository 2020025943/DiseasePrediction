{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import mutual_info_classif as MIF\n",
    "import random\n",
    "import os\n",
    "# chain()可以把一组迭代对象串联起来，形成一个更大的迭代器\n",
    "from itertools import chain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import optimizers, layers, losses, metrics\n",
    "from tensorflow.keras.initializers import glorot_normal, he_normal\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, multiply, Flatten, Concatenate\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from deepctr.inputs import get_dense_input, create_embedding_matrix, embedding_lookup, get_dense_input, varlen_embedding_lookup, \\\n",
    "    get_varlen_pooling_list, mergeDict\n",
    "from deepctr.layers.sequence import SequencePoolingLayer, Transformer, AttentionSequencePoolingLayer\n",
    "\n",
    "from deepctr.feature_column import  SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names, build_input_features, get_linear_logit, DEFAULT_GROUP_NAME, input_from_feature_columns\n",
    "from deepctr.layers.core import PredictionLayer, DNN\n",
    "from deepctr.layers.interaction import FM, FEFMLayer, BiInteractionPooling, AFMLayer, CIN, InteractingLayer, FwFMLayer, InnerProductLayer, OutterProductLayer, FGCNNLayer, CrossNet,  CrossNetMix\n",
    "from deepctr.layers.utils import concat_func, add_func, Hash, NoMask, combined_dnn_input, reduce_sum, softmax\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import Zeros, glorot_normal, RandomNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepFEFMA(linear_feature_columns, dnn_feature_columns, use_fefm=True,\n",
    "             dnn_hidden_units=(256, 128, 64), l2_reg_linear=0.00001, l2_reg_embedding_feat=0.00001,\n",
    "             l2_reg_embedding_field=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0.0,\n",
    "             exclude_feature_embed_in_dnn=False,\n",
    "             use_linear=True, use_fefm_embed_in_dnn=True, dnn_activation='relu', dnn_use_bn=False, task='binary'):\n",
    "    features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    inputs_list = list(features.values())\n",
    "\n",
    "    linear_logit = get_linear_logit(features, linear_feature_columns, l2_reg=l2_reg_linear, seed=seed, prefix='linear')\n",
    "\n",
    "    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "                                                                        l2_reg_embedding_feat,\n",
    "                                                                        seed, support_group=True)\n",
    "\n",
    "    fefm_interaction_embedding = concat_func([FEFMLayer(\n",
    "        regularizer=l2_reg_embedding_field)(concat_func(v, axis=1))\n",
    "                                              for k, v in group_embedding_dict.items() if k in [DEFAULT_GROUP_NAME]],\n",
    "                                             axis=1)\n",
    "\n",
    "    dnn_input = combined_dnn_input(list(chain.from_iterable(group_embedding_dict.values())), dense_value_list)\n",
    "\n",
    "    # if use_fefm_embed_in_dnn is set to False it is Ablation4 (Use false only for Ablation)\n",
    "    if use_fefm_embed_in_dnn:\n",
    "        if exclude_feature_embed_in_dnn:\n",
    "            # Ablation3: remove feature vector embeddings from the DNN input\n",
    "            dnn_input = fefm_interaction_embedding\n",
    "        else:\n",
    "            # No ablation\n",
    "            dnn_input = concat_func([dnn_input, fefm_interaction_embedding], axis=1)\n",
    "\n",
    "    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)\n",
    "\n",
    "    dnn_logit = tf.keras.layers.Dense(\n",
    "        1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_out)\n",
    "\n",
    "    fefm_logit = tf.keras.layers.Lambda(lambda x: reduce_sum(x, axis=1, keep_dims=True))(fefm_interaction_embedding)\n",
    "    \n",
    "    linear_logit = Dense(1, use_bias=False)(linear_logit)\n",
    "    fefm_logit = Dense(1, use_bias=False)(fefm_logit)\n",
    "    dnn_logit = Dense(1, use_bias=False)(dnn_logit)\n",
    "    \n",
    "    if len(dnn_hidden_units) == 0 and use_fefm is False and use_linear is True:  # only linear\n",
    "        final_logit = linear_logit\n",
    "    elif len(dnn_hidden_units) == 0 and use_fefm is True and use_linear is True:  # linear + FEFM\n",
    "        final_logit = tf.keras.layers.add([linear_logit, fefm_logit])\n",
    "    elif len(dnn_hidden_units) > 0 and use_fefm is False and use_linear is True:  # linear +　Deep # Ablation1\n",
    "        final_logit = tf.keras.layers.add([linear_logit, dnn_logit])\n",
    "    elif len(dnn_hidden_units) > 0 and use_fefm is True and use_linear is True:  # linear + FEFM + Deep\n",
    "        final_logit = tf.keras.layers.add([linear_logit, fefm_logit, dnn_logit])\n",
    "    elif len(dnn_hidden_units) == 0 and use_fefm is True and use_linear is False:  # only FEFM (shallow)\n",
    "        final_logit = fefm_logit\n",
    "    elif len(dnn_hidden_units) > 0 and use_fefm is False and use_linear is False:  # only Deep\n",
    "        final_logit = dnn_logit\n",
    "    elif len(dnn_hidden_units) > 0 and use_fefm is True and use_linear is False:  # FEFM + Deep # Ablation2\n",
    "        final_logit = tf.keras.layers.add([fefm_logit, dnn_logit])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    output = PredictionLayer(task)(final_logit)\n",
    "    model = tf.keras.models.Model(inputs=inputs_list, outputs=output)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), \n",
    "               loss=losses.BinaryCrossentropy(), \n",
    "               metrics=['AUC', 'binary_accuracy', 'Precision', 'Recall'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "lrs = [1]\n",
    "for i in lrs:\n",
    "    metric = []\n",
    "    for j in range(1):\n",
    "        model = DeepFEFMA(linear_feature_columns, dnn_feature_columns, use_fefm=True,\n",
    "             dnn_hidden_units=(256, 128, 64), l2_reg_linear=0.00001, l2_reg_embedding_feat=0.00001,\n",
    "             l2_reg_embedding_field=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0.0,\n",
    "             exclude_feature_embed_in_dnn=False,\n",
    "             use_linear=True, use_fefm_embed_in_dnn=True, dnn_activation='relu', dnn_use_bn=False, task='binary'):\n",
    "    features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
    "        input_train = deepfm_train\n",
    "        model.fit(input_train,\n",
    "                        label,\n",
    "                        validation_split=0.2,\n",
    "                        epochs=100,\n",
    "                        batch_size=128,\n",
    "                        shuffle = False,\n",
    "                        verbose = 1, \n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],)\n",
    "        input_test = deepfm_test\n",
    "        ans_mtx = model.predict(input_test, batch_size=100)\n",
    "        loss, auc, acc, pre, rec = model.evaluate(input_test, label1)\n",
    "        metric.append([loss, auc, acc, pre, rec])\n",
    "    metric = np.array(metric)\n",
    "    print(metric)\n",
    "    metrics.append(metric)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)\n",
    "print(*zip(lrs, np.mean(metrics, axis = 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
