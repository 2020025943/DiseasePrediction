{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "n_episodes = 100\n",
    "n_way = 10\n",
    "n_shot = 5\n",
    "n_query = 15\n",
    "n_examples = 350\n",
    "lr = 0.001\n",
    "width, height, channels = 84, 84, 3\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps = 2000, decay_rate = 0.5)\n",
    "optimizer = Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 350, 84, 84, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = np.load('train.npy')\n",
    "n_classes = train_dataset.shape[0]\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2c5514e0f420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#         return out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPrototypical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m     35\u001b[0m     \u001b[0mImplemenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mPrototypical\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "    \"\"\"\n",
    "    Calculate euclidian distance between two 3D tensors.\n",
    "\n",
    "    Args:\n",
    "        x (tf.Tensor):\n",
    "        y (tf.Tensor):\n",
    "\n",
    "    Returns (tf.Tensor): 2-dim tensor with distances.\n",
    "\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.abs(x-y), axis = 2)\n",
    "# def regularized_padded_conv(*args, **kwargs):\n",
    "#     return layers.Conv2D(*args, **kwargs, padding='same', use_bias=False,\n",
    "#                          kernel_initializer='he_normal',\n",
    "#                          kernel_regularizer=regularizers.l2(5e-4))\n",
    "# class SpatialAttention(layers.Layer):\n",
    "#     def __init__(self, kernel_size=7):\n",
    "#         super(SpatialAttention, self).__init__()\n",
    "#         self.conv1 = regularized_padded_conv(1, kernel_size=kernel_size, strides=1, activation='sigmoid')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         avg_out = tf.reduce_mean(inputs, axis=3)\n",
    "#         max_out = tf.reduce_max(inputs, axis=3)\n",
    "#         out = tf.stack([avg_out, max_out], axis=-1)             # 创建一个维度,拼接到一起concat。\n",
    "#         out = self.conv1(out)\n",
    "\n",
    "#         return out\n",
    "class Prototypical(Model):\n",
    "    \"\"\"\n",
    "    Implemenation of Prototypical Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_support, n_query, w, h, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_support (int): number of support examples.\n",
    "            n_query (int): number of query examples.\n",
    "            w (int): image width .\n",
    "            h (int): image height.\n",
    "            c (int): number of channels.\n",
    "        \"\"\"\n",
    "        super(Prototypical, self).__init__()\n",
    "        self.w, self.h, self.c = w, h, c\n",
    "\n",
    "        # Encoder as ResNet like CNN with 4 blocks\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)),\n",
    "\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D((2, 2)), Flatten()]\n",
    "        )\n",
    "#         self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same')\n",
    "#         self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same')\n",
    "#         self.conv3 = tf.keras.layers.Conv2D(filters=1, kernel_size=3, padding='same')\n",
    "#         self.drop = tf.keras.layers.Dropout()\n",
    "#     def feature_attention(self, z_prototypes):\n",
    "#         score = tf.nn.relu(self.conv1(z_prototypes))\n",
    "#         score = tf.nn.relu(self.conv2(score)) \n",
    "#         score = self.drop(score)\n",
    "#         score = tf.nn.relu(self.conv_3(score))\n",
    "#         score = tf.reshape(score, [n_class, n_support, z.shape[-1]])\n",
    "#         return score\n",
    "\n",
    "    def call(self, support, query):\n",
    "        n_class = support.shape[0]\n",
    "        n_support = support.shape[1]\n",
    "        n_query = query.shape[1]\n",
    "#         fea_att_score = feature_attention(self, support, n_class, n_support):\n",
    "        y = np.tile(np.arange(n_class)[:, np.newaxis], (1, n_query))\n",
    "        y_onehot = tf.cast(tf.one_hot(y, n_class), tf.float32)\n",
    "\n",
    "        # correct indices of support samples (just natural order)\n",
    "        target_inds = tf.reshape(tf.range(n_class), [n_class, 1])\n",
    "        target_inds = tf.tile(target_inds, [1, n_query])\n",
    "\n",
    "        # merge support and query to forward through encoder\n",
    "        cat = tf.concat([\n",
    "            tf.reshape(support, [n_class * n_support,\n",
    "                                 self.w, self.h, self.c]),\n",
    "            tf.reshape(query, [n_class * n_query,\n",
    "                               self.w, self.h, self.c])], axis=0)\n",
    "        z = self.encoder(cat)\n",
    "\n",
    "        # Divide embedding into support and query\n",
    "        z_prototypes = tf.reshape(z[:n_class * n_support],\n",
    "                                  [n_class, n_support, z.shape[-1]])\n",
    "        # Prototypes are means of n_support examples\n",
    "        z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)\n",
    "        z_query = z[n_class * n_support:]\n",
    "\n",
    "        # Calculate distances between query and prototypes\n",
    "        dists = calc_euclidian_dists(z_query, z_prototypes)\n",
    "\n",
    "        # log softmax of calculated distances\n",
    "        log_p_y = tf.nn.log_softmax(-dists, axis=-1)\n",
    "        log_p_y = tf.reshape(log_p_y, [n_class, n_query, -1])\n",
    "        \n",
    "        loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_onehot, log_p_y), axis=-1), [-1]))\n",
    "        eq = tf.cast(tf.equal(\n",
    "            tf.cast(tf.argmax(log_p_y, axis=-1), tf.int32), \n",
    "            tf.cast(y, tf.int32)), tf.float32)\n",
    "        acc = tf.reduce_mean(eq)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/BaiduNetdiskDownload/train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5f59f579ce74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/BaiduNetdiskDownload/train.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/BaiduNetdiskDownload/train.npy'"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "model = Prototypical(n_shot, n_query, width, height, channels)\n",
    "for ep in range(n_epochs):\n",
    "    all_loss = 0\n",
    "    all_acc = 0\n",
    "    for epi in range(n_episodes):\n",
    "        epi_classes = np.random.permutation(n_classes)[:n_way]\n",
    "        support = np.zeros([n_way, n_shot, height, width, channels], dtype=np.float32)\n",
    "        query = np.zeros([n_way, n_query, height, width, channels], dtype=np.float32)\n",
    "        for i, epi_cls in enumerate(epi_classes):\n",
    "            selected = np.random.permutation(n_examples)[:n_shot + n_query]\n",
    "            support[i] = train_dataset[epi_cls, selected[:n_shot]]\n",
    "            query[i] = train_dataset[epi_cls, selected[n_shot:]]\n",
    "            # support = np.expand_dims(support, axis=-1)\n",
    "            # query = np.expand_dims(query, axis=-1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = model(support, query)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(\n",
    "            zip(gradients, model.trainable_variables))\n",
    "        if (epi+1) % 50 == 0:\n",
    "            print('[epoch {}/{}, episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(ep+1, n_epochs, epi+1, n_episodes, loss, acc))\n",
    "        all_loss += loss.numpy()\n",
    "        all_acc += acc.numpy()\n",
    "    train_loss.append(all_loss / n_episodes)\n",
    "    train_acc.append(all_acc / n_episodes)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Dataset\n",
    "test_dataset = np.load('test.npy')\n",
    "n_test_classes = test_dataset.shape[0]\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_episodes = 600\n",
    "n_test_way = 5\n",
    "n_test_shot = 5\n",
    "n_test_query = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing...')\n",
    "avg_acc = 0.\n",
    "all_loss = 0\n",
    "all_acc = 0\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for epi in range(n_test_episodes):\n",
    "    epi_classes = np.random.permutation(n_test_classes)[:n_test_way]\n",
    "    support = np.zeros([n_test_way, n_test_shot, height, width, channels], dtype=np.float32)\n",
    "    query = np.zeros([n_test_way, n_test_query, height, width, channels], dtype=np.float32)\n",
    "    for i, epi_cls in enumerate(epi_classes):\n",
    "        selected = np.random.permutation(n_examples)[:n_test_shot + n_test_query]\n",
    "        support[i] = test_dataset[epi_cls, selected[:n_test_shot]]\n",
    "        query[i] = test_dataset[epi_cls, selected[n_test_shot:]]\n",
    "    loss, acc = model(support, query)\n",
    "    avg_acc += acc\n",
    "    all_loss += loss.numpy()\n",
    "    all_acc += acc.numpy()\n",
    "    if (epi+1) % 50 == 0:\n",
    "        print('[test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_test_episodes, loss, acc))\n",
    "        test_loss.append(all_loss / 50)\n",
    "        test_acc.append(all_acc / 50)\n",
    "        all_acc = 0\n",
    "        all_loss = 0\n",
    "avg_acc /= n_test_episodes\n",
    "print('Average Test Accuracy: {:.5f}'.format(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
