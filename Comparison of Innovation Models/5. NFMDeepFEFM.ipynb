{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import mutual_info_classif as MIF\n",
    "import random\n",
    "import os\n",
    "# chain()可以把一组迭代对象串联起来，形成一个更大的迭代器\n",
    "from itertools import chain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import optimizers, layers, losses, metrics\n",
    "from tensorflow.keras.initializers import glorot_normal, he_normal\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, multiply, Flatten, Concatenate\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from deepctr.inputs import get_dense_input, create_embedding_matrix, embedding_lookup, get_dense_input, varlen_embedding_lookup, \\\n",
    "    get_varlen_pooling_list, mergeDict\n",
    "from deepctr.layers.sequence import SequencePoolingLayer, Transformer, AttentionSequencePoolingLayer\n",
    "\n",
    "from deepctr.feature_column import  SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names, build_input_features, get_linear_logit, DEFAULT_GROUP_NAME, input_from_feature_columns\n",
    "from deepctr.layers.core import PredictionLayer, DNN\n",
    "from deepctr.layers.interaction import FM, FEFMLayer, BiInteractionPooling, AFMLayer, CIN, InteractingLayer, FwFMLayer, InnerProductLayer, OutterProductLayer, FGCNNLayer, CrossNet,  CrossNetMix\n",
    "from deepctr.layers.utils import concat_func, add_func, Hash, NoMask, combined_dnn_input, reduce_sum, softmax\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import Zeros, glorot_normal, RandomNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFMDeepFEFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),\n",
    "        l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_embedding_feat=0.00001,\n",
    "             l2_reg_embedding_field=0.00001,l2_reg_dnn=0, seed=1024, bi_dropout=0,\n",
    "        dnn_dropout=0, dnn_activation='relu', task='binary'):\n",
    "\n",
    "    features = build_input_features(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    inputs_list = list(features.values())\n",
    "\n",
    "    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',\n",
    "                                    l2_reg=l2_reg_linear)\n",
    "\n",
    "#     sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "#                                                                          l2_reg_embedding, seed)\n",
    "\n",
    "\n",
    "    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "                                                                        l2_reg_embedding_feat,\n",
    "                                                                        seed, support_group=True)\n",
    "    sparse_embedding_list = list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    fm_input = concat_func(sparse_embedding_list, axis=1)\n",
    "\n",
    "    fefm_interaction_embedding = concat_func([FEFMLayer(\n",
    "        regularizer=l2_reg_embedding_field)(concat_func(v, axis=1))\n",
    "                                              for k, v in group_embedding_dict.items() if k in [DEFAULT_GROUP_NAME]],axis=1)\n",
    "\n",
    "\n",
    "    fefm_logit = tf.keras.layers.Lambda(lambda x: reduce_sum(x, axis=1, keep_dims=True))(fefm_interaction_embedding)\n",
    "    bi_out = BiInteractionPooling()(fm_input)\n",
    "    if bi_dropout:\n",
    "        bi_out = tf.keras.layers.Dropout(bi_dropout)(bi_out, training=None)\n",
    "    dnn_input = combined_dnn_input([fefm_logit], dense_value_list)\n",
    "    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)\n",
    "    dnn_logit = tf.keras.layers.Dense(\n",
    "        1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)\n",
    "\n",
    "    final_logit = add_func([linear_logit, dnn_logit])\n",
    "\n",
    "    output = PredictionLayer(task)(final_logit)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs_list, outputs=output)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), \n",
    "               loss=losses.BinaryCrossentropy(), \n",
    "               metrics=['AUC', 'binary_accuracy', 'Precision', 'Recall'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "lrs = [1]\n",
    "for i in lrs:\n",
    "    metric = []\n",
    "    for j in range(1):\n",
    "        model = NFMDeepFEFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),\n",
    "        l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_embedding_feat=0.00001,\n",
    "             l2_reg_embedding_field=0.00001,l2_reg_dnn=0, seed=1024, bi_dropout=0,\n",
    "        dnn_dropout=0, dnn_activation='relu', task='binary')\n",
    "        input_train = deepfm_train\n",
    "        model.fit(input_train,\n",
    "                        label,\n",
    "                        validation_split=0.2,\n",
    "                        epochs=100,\n",
    "                        batch_size=128,\n",
    "                        shuffle = False,\n",
    "                        verbose = 1, \n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],)\n",
    "        input_test = deepfm_test\n",
    "        ans_mtx = model.predict(input_test, batch_size=100)\n",
    "        loss, auc, acc, pre, rec = model.evaluate(input_test, label1)\n",
    "        metric.append([loss, auc, acc, pre, rec])\n",
    "    metric = np.array(metric)\n",
    "    print(metric)\n",
    "    metrics.append(metric)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)\n",
    "print(*zip(lrs, np.mean(metrics, axis = 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
